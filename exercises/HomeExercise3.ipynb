{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro_ws2024/blob/main/exercises/HomeExercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Exericse 3: Hyperparameters and Evaluation\n",
        "In this third home exercise, you will use the knowledge from Tutorial 4 to experiment with hyperparameters, create a test set, and evaluate your final model on the created test set.\n",
        "\n",
        "In this notebook, please complete all instructions starting with ðŸ‘‹ âš’ in the code cell after the sign or provide your analysis in the text cell after the sign."
      ],
      "metadata": {
        "id": "N4_fSCGEAFZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Distilbert: Hyperparameters and Evaluation**"
      ],
      "metadata": {
        "id": "vnXNn2eL1jnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the code of Tutorial 4 to load and fine-tune the `distilbert-base-cased`model on the small subset of the `imdb`Movie Review Dataset. For convenience, the code of Tutorial 4 required for this exercise is already provided in the code cells below.\n",
        "\n",
        "ðŸ‘‹ âš’ When creating the dataset splits in the code cell below, additionally create a test set to be used after thet training. Make sure that your test set does not contain any of the sentences contained in the training or validation set and is approximately of the same size as the validation set."
      ],
      "metadata": {
        "id": "YHR4hkXKvd87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install accelerate --upgrade"
      ],
      "metadata": {
        "id": "kxwz_cPyW4lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "# we had loaded the imdb dataset already above - if not, outcomment this line\n",
        "# Make sure you have the right tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed on CPU\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:100]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=42).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=42).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "1z4TAwF0v-8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ For this exercise, we will use the Hugging Face Trainer class to play with hyperparamters. Try to find a set of hyperparameter settings that achieves the highest possilbe accuracy on the **validation set** with the small dataset and model in this setup.\n",
        "\n",
        "**Optional:** If you want to follow a more systematic route, feel free to use available frameworks for hyperparameter optimization, such as [Optuna](https://optuna.org/)."
      ],
      "metadata": {
        "id": "RZzQz_y67TCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import set_seed\n",
        "\n",
        "set_seed(24)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=2)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=24\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "KehTDYh37cKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2XufI7yq8c7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Change the following code cell in a way that not only a single sentence is evaluated on your trained model (!make sure to use the correct checkpoint!) but the evaluation is performaned on the entire newly created test set.\n",
        "\n",
        "This might also be a good occassion to get familiar with the [Hugging Face documentation and tutorials](https://huggingface.co/docs/transformers/index)."
      ],
      "metadata": {
        "id": "AIo86-js8ZrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"I love this movie!\"\n",
        "\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-40\")\n",
        "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
        "prediction = torch.argmax(fine_tuned_model(**model_inputs).logits)\n",
        "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
      ],
      "metadata": {
        "id": "XCiYFTr28Zz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}