{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fubotz/cl_intro_ws2024/blob/main/HomeExercise2_Fabian_SCHAMBECK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Exericse 2: Word Embeddings\n",
        "In this second home exercise, you will use the knowledge from Tutorial 3 to perform a more systematic evaluation of embeddings based on a small analogy dataset.\n",
        "\n",
        "In this notebook, please complete all instructions starting with üëã ‚öí in the code cell after the sign or provide your analysis in the text cell after the sign."
      ],
      "metadata": {
        "id": "N4_fSCGEAFZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word2Vec Analogy-based Evaluation**"
      ],
      "metadata": {
        "id": "vnXNn2eL1jnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to load the pretrained embeddings and the dataset. The dataset can be found on [GitHub](https://github.com/dgromann/cl_intro_ws2024/blob/main/exercises/HomeExercise2.txt) and will be loaded directly from there."
      ],
      "metadata": {
        "id": "YHR4hkXKvd87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/dgromann/cl_intro_ws2024/raw/main/word2vec_embeddings.bin\n",
        "!wget !wget https://raw.githubusercontent.com/dgromann/cl_intro_ws2024/master/exercises/HomeExercise2.txt"
      ],
      "metadata": {
        "id": "kxwz_cPyW4lG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aeb891b-573b-43c1-f33e-b3b8d5ac2c10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-24 19:05:32--  https://github.com/dgromann/cl_intro_ws2024/raw/main/word2vec_embeddings.bin\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dgromann/cl_intro_ws2024/main/word2vec_embeddings.bin [following]\n",
            "--2024-11-24 19:05:33--  https://raw.githubusercontent.com/dgromann/cl_intro_ws2024/main/word2vec_embeddings.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96769269 (92M) [application/octet-stream]\n",
            "Saving to: ‚Äòword2vec_embeddings.bin.1‚Äô\n",
            "\n",
            "word2vec_embeddings 100%[===================>]  92.29M   191MB/s    in 0.5s    \n",
            "\n",
            "2024-11-24 19:05:36 (191 MB/s) - ‚Äòword2vec_embeddings.bin.1‚Äô saved [96769269/96769269]\n",
            "\n",
            "--2024-11-24 19:05:36--  http://!wget/\n",
            "Resolving !wget (!wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‚Äò!wget‚Äô\n",
            "--2024-11-24 19:05:36--  https://raw.githubusercontent.com/dgromann/cl_intro_ws2024/master/exercises/HomeExercise2.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272272 (266K) [text/plain]\n",
            "Saving to: ‚ÄòHomeExercise2.txt.1‚Äô\n",
            "\n",
            "HomeExercise2.txt.1 100%[===================>] 265.89K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-11-24 19:05:37 (4.77 MB/s) - ‚ÄòHomeExercise2.txt.1‚Äô saved [272272/272272]\n",
            "\n",
            "FINISHED --2024-11-24 19:05:37--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 266K in 0.05s (4.77 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to load the model with gensim so that we can access the embeddings."
      ],
      "metadata": {
        "id": "If_-whLgv9kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin\", binary=True)     # load pre-trained Word2Vec embeddings in binary format --> .bin == binary encoded file"
      ],
      "metadata": {
        "id": "1z4TAwF0v-8x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we need to open the HomeExercise2.txt file that contains analogy pairs."
      ],
      "metadata": {
        "id": "88Q9RW2ZwHkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = open(\"HomeExercise2.txt\", \"r\")\n",
        "analogy_lines = analogy.readlines()     # .readlines() method used to read lines and return them as list of strings; each string represents a line from file\n",
        "\n",
        "\n",
        "# NB: anology:\n",
        "# A : B :: C : D"
      ],
      "metadata": {
        "id": "saqImWrswKYJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at the first few lines, the following code can be used. The analogies are grouped by categories that is indicated on the line before the anlogies are listed with a colon :. The last and fourth element of the line represents the true result we will use to evaluate the embedding model."
      ],
      "metadata": {
        "id": "X8fwdQjOxDR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_no = 0\n",
        "for line in analogy_lines:\n",
        "    line_no += 1\n",
        "    print(f\"Line number {line_no} with analogy {line}\")\n",
        "    if line_no == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "yKHVQpGbxGc7",
        "outputId": "ff61ca07-ab7c-4507-c9bb-325b8849f9cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line number 1 with analogy : capital-common-countries\n",
            "\n",
            "Line number 2 with analogy Athens Greece Baghdad Iraq\n",
            "\n",
            "Line number 3 with analogy Athens Greece Berlin Germany\n",
            "\n",
            "Line number 4 with analogy Athens Greece Cairo Egypt\n",
            "\n",
            "Line number 5 with analogy Athens Greece Canberra Australia\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Systematically evaluate this simple word embedding model based on the entire analogy dataset. To do this:\n",
        "\n",
        "\n",
        "*   Use the analogy function from Tutorial 3 to obtain 'd'\n",
        "*   Compare 'd' with the true result from the `HomeExercise2.txt` file\n",
        "*   Calculate the accuracy for all analogies (how many times out of all attempts did the embedding model provide the correct result)\n",
        "*   Calculate the accuracy for each analogy category separately\n",
        "\n",
        "When parsing the file, pay attention to the lines indicated with the colon : that represent the analogy categories and not analogies.\n"
      ],
      "metadata": {
        "id": "a3xklUvEXbd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "# Example: Athens is to Greece as Baghdad is to ?\n",
        "# True result from file: Iraq\n",
        "# Model result: also Iraq?\n",
        "\n",
        "\n",
        "def analogy(a, b, c):\n",
        "    result = model.most_similar(positive=[b, c], negative=[a], topn=1)      # calculate the analogy: b - a + c = d\n",
        "    return result[0][0]\n",
        "\n",
        "# Initialize variables to track accuracy of prediction\n",
        "total_attempts = 0      # tracks total number of analogies attempted\n",
        "total_correct = 0       # total number of correctly predicted analogies\n",
        "category_attempts = {}      # dict to track total number of attempts for each category\n",
        "category_correct = {}       # dict to track total number of correct predictions for each category\n",
        "\n",
        "\n",
        "with open(\"HomeExercise2.txt\", \"r\") as file:\n",
        "    current_category = None     # track current analogy category\n",
        "\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if the line is a category header\n",
        "        if line.startswith(\":\"):\n",
        "            current_category = line[1:].strip()     # extract category name without colon at index [0]\n",
        "            # Initialize counters for the new category\n",
        "            category_attempts[current_category] = 0\n",
        "            category_correct[current_category] = 0\n",
        "\n",
        "        else:\n",
        "            # Split the other lines into sperate strings (a, b, c, d_true)\n",
        "            words = line.split()\n",
        "            if len(words) == 4:     # if number of words per line == 4\n",
        "                a, b, c, d_true = words     # assign a, b, c, d_true respectively to each word in that line\n",
        "\n",
        "            # Use the analogy function to get the predicted result\n",
        "            predicted_d = analogy(a, b, c)\n",
        "\n",
        "            # Update overall counts\n",
        "            total_attempts += 1\n",
        "            category_attempts[current_category] += 1\n",
        "\n",
        "            if predicted_d == d_true:\n",
        "                total_correct += 1\n",
        "                category_correct[current_category] += 1\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "overall_accuracy = total_correct / total_attempts if total_attempts > 0 else 0      # if-statement avoids division by 0\n",
        "print(f\"Overall accuracy of the model: {overall_accuracy:.2f}\")\n",
        "\n",
        "# Calculate and print category-specific accuracies\n",
        "for category, attempts in category_attempts.items():\n",
        "    correct = category_correct[category]\n",
        "    accuracy = correct / attempts if attempts > 0 else 0\n",
        "    print(f\"Accuracy for {category}: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# print(analogy(\"Athens\", \"Greece\", \"Baghdad\"))       # Greece(b) - Athens(a) + Baghdad(c) = Iraq(d); outputs d == \"Iraqi\" because analogy relies on word embeddings from pre-trained model\n"
      ],
      "metadata": {
        "id": "knGLoU0kXaZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a662c5-e79f-420e-988d-5177f2533a1b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of the model: 0.75\n",
            "Accuracy for capital-common-countries: 0.87\n",
            "Accuracy for capital-world: 0.90\n",
            "Accuracy for currency: 0.00\n",
            "Accuracy for city-in-state: 0.79\n",
            "Accuracy for family: 0.93\n",
            "Accuracy for gram1-adjective-to-adverb: 0.30\n",
            "Accuracy for gram2-opposite: 0.54\n",
            "Accuracy for gram3-comparative: 0.91\n",
            "Accuracy for gram4-superlative: 0.88\n",
            "Accuracy for gram5-present-participle: 0.78\n",
            "Accuracy for gram6-nationality-adjective: 0.96\n",
            "Accuracy for gram7-past-tense: 0.69\n",
            "Accuracy for gram8-plural: 0.87\n",
            "Accuracy for gram9-plural-verbs: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Word2Vec:\n",
        "\n",
        "The results show that the model achieved an overall accuracy of 75%, meaning it correctly resolved three-quarters of the analogies. Categories with particularly high accuracy rates, which stand out, include capital-world, family, and gram6-nationality-adjective."
      ],
      "metadata": {
        "id": "AYD1hMjV35s5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparison: GloVe Analogy-based Evaluation**"
      ],
      "metadata": {
        "id": "bM-lxrQV15lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step will consist of comparing this very small word2vec embedding model with a different small but more powerfull model available in gensim.\n",
        "\n",
        "All models and corpora available in gensim can be found [here](https://github.com/piskvorky/gensim-data).\n",
        "\n",
        "Since this model is considerably bigger than the tiny word2vec model, it takes some time to load when you run the following code cell."
      ],
      "metadata": {
        "id": "KN5pGa7JzT9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "print(type(model))"
      ],
      "metadata": {
        "id": "noalz14AzkQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can then be used exactly the same as the word2vec model, since gensim standardizes model access."
      ],
      "metadata": {
        "id": "kEWEOytj0hrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_glove[\"bread\"]"
      ],
      "metadata": {
        "id": "NmyLiSoF0s0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí  Run the same systematic analysis for this gensim model as for the word2vec model above. Which model performs better overall and in specific categories?"
      ],
      "metadata": {
        "id": "N-eMSsyO1PMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "dZk0eA3B1ZSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visual Comparison**\n",
        "\n",
        "As a final step, use the visualization from Tutorial 3 to visually output the two models based on the following words."
      ],
      "metadata": {
        "id": "r1b7CPN-2Clq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí  ‚ùì Do the clusters (groupings of embeddings) in the GloVe visualization differ substantially from the clusters in the word2vec visualization from Tutorial 3?"
      ],
      "metadata": {
        "id": "QWg0FNJG1aY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def display_pca_scatterplot(model, words):\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "WaHuaINH2UJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(model_glove,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'water',\n",
        "                         'hamburger', 'pizza',  'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'lizard',\n",
        "                         'france', 'germany', 'hungary',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ],
      "metadata": {
        "id": "h5C4yZsN2lAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Provide your answer to the question on the clusters here.**"
      ],
      "metadata": {
        "id": "lbxcCxRB4Rvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bias in Embeddings**\n",
        "\n",
        "Language models and also embedding models tend to reflect on bias that is present in the textual data they were trained on. This can also be analyzed with embeddings by explicitly testing biased analogies.\n",
        "\n",
        "For instance, man is to doctor as woman is to ?\n",
        "\n",
        "The bias here is that professions tend to be assigned a specific gender, e.g. men are doctors and women are nurses.\n",
        "\n",
        "The same is true for cultures and cultural bias, e.g. Bratwurst or Sauerkraut and Germany.\n",
        "\n"
      ],
      "metadata": {
        "id": "fR022k6924H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = model_glove.most_similar(positive=[\"doctor\", \"woman\"], negative=[\"man\"], topn=3)\n",
        "print(f\"man is to doctor as woman is to {result1}\")\n",
        "result2 = model_glove.most_similar(positive=[\"bratwurst\", \"france\"], negative=[\"germany\"], topn=3)\n",
        "print(f\"Germany is to Bratwurst as France is to {result2}\")"
      ],
      "metadata": {
        "id": "bdEoibPg4Ylz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Try to come up with two biased analogies yourself and test if the GloVe and word2vec models suffers from this type of bias. Please try to be creative and do not just change woman to girl and man to boy or something similar."
      ],
      "metadata": {
        "id": "9S2HoyVE7hL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your biased analogies on both models here"
      ],
      "metadata": {
        "id": "IVvhsv5u7wHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}