{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84215528a2ba46dfaac1c55383bea363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27d19fb9255c4583b2b1de84a37c36c2",
              "IPY_MODEL_ef8e5e4e8edc41358387d8aa158f274c",
              "IPY_MODEL_905b0f37cc73439e908529443bcc6e8f"
            ],
            "layout": "IPY_MODEL_21b04cce8c8f42e6a15c875079158bee"
          }
        },
        "27d19fb9255c4583b2b1de84a37c36c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1769bf22a5464822bf24f405ded5c8fa",
            "placeholder": "​",
            "style": "IPY_MODEL_16d350da50bd49b59bdfd7910e080cd8",
            "value": "Map: 100%"
          }
        },
        "ef8e5e4e8edc41358387d8aa158f274c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c80abc8dcf44ead85b9a21262cd58cd",
            "max": 492,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd695e19f29142d290152586082fe50a",
            "value": 492
          }
        },
        "905b0f37cc73439e908529443bcc6e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a4ec51c39164843ab923333e116a1ba",
            "placeholder": "​",
            "style": "IPY_MODEL_4073116836a34c808e535cdea9bdf82e",
            "value": " 492/492 [00:00&lt;00:00, 9730.14 examples/s]"
          }
        },
        "21b04cce8c8f42e6a15c875079158bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1769bf22a5464822bf24f405ded5c8fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d350da50bd49b59bdfd7910e080cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c80abc8dcf44ead85b9a21262cd58cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd695e19f29142d290152586082fe50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a4ec51c39164843ab923333e116a1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4073116836a34c808e535cdea9bdf82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fubotz/ICL_2024W/blob/main/FinalProject_Fabian_SCHAMBECK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICL Final Project: Finetuning a Pretrained Multilingual Model for Cognate Detection\n",
        "\n",
        "Model: xlm-roberta-base\n",
        "\n",
        "Dataset: custom dataset containing en-fr cognates (Frossard et al.)\n",
        "\n",
        "Method: < mask > approach"
      ],
      "metadata": {
        "id": "N4_fSCGEAFZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install -U evaluate\n",
        "!pip install -U transformers\n",
        "!pip install -U torch"
      ],
      "metadata": {
        "id": "v1II96WS0W56",
        "outputId": "33bbd546-da57-4d77-c567-da217f821172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "Successfully installed transformers-4.48.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "194927f6987047c697e95142e8a5e4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YwUn733bt7lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset ##"
      ],
      "metadata": {
        "id": "5HQHtDU0_JW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/fubotz/ICL_2024W/refs/heads/main/word_pairs.json        # dataset taken from Frossard et al."
      ],
      "metadata": {
        "id": "kcC8-tRZ-5yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"word_pairs.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "print(dataset[:10])"
      ],
      "metadata": {
        "id": "JTOGfoFS_MaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the dataset to a dictionary format with separate lists for English and French words\n",
        "dataset_dict = {\n",
        "    \"word_en\": [list(pair.keys())[0] for pair in dataset],      # Extract English words\n",
        "    \"word_fr\": [list(pair.values())[0] for pair in dataset]     # Extract French words\n",
        "}\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Verify structure\n",
        "print(dataset, \"\\n\")\n",
        "print(dataset[:10])"
      ],
      "metadata": {
        "id": "qmP-UoqdJmrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model ##"
      ],
      "metadata": {
        "id": "qyBJxKpF-9wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Model for <mask> approach\n",
        "pretrained_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Freeze and unfreeze x encoder layers\n",
        "for param in pretrained_model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in pretrained_model.base_model.encoder.layer[-5:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "vQlmoRnEQykz",
        "outputId": "f150e210-f3bd-4692-8910-93781cba521c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Dataset ##"
      ],
      "metadata": {
        "id": "51DQW-EYKURY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes input words, replaces the French word with <mask>,\n",
        "    and assigns ALL subword tokens (BytePair Encoding) of the correct target\n",
        "    word as labels.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A batch of English-French cognate pairs in dictionary format:\n",
        "                         {\"word_en\": [...], \"word_fr\": [...]}\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - input_ids: Tokenized sentences with <mask>\n",
        "            - attention_mask: Mask indicating valid tokens\n",
        "            - labels: Correct token IDs for the French word at the <mask> position\n",
        "    \"\"\"\n",
        "    # Construct masked input sentences\n",
        "    masked_sentences = [\n",
        "        f\"In English, the word is {word_en}. En Français, le mot est {tokenizer.mask_token}.\"\n",
        "        for word_en in examples[\"word_en\"]\n",
        "    ]\n",
        "\n",
        "    # Tokenize input sentences\n",
        "    model_inputs = tokenizer(masked_sentences, max_length=20, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Find <mask> token indices\n",
        "    mask_indices = [\n",
        "        (torch.tensor(input_ids) == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].tolist()\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    # Tokenize target words (French cognates) WITHOUT special tokens\n",
        "    target_tokens = tokenizer(examples[\"word_fr\"], add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Initialize label tensor with -100 (ignored positions)\n",
        "    model_inputs[\"labels\"] = [[-100] * len(input_ids) for input_ids in model_inputs[\"input_ids\"]]\n",
        "\n",
        "    # Assign correct token IDs at the <mask> position\n",
        "    for i, mask_pos in enumerate(mask_indices):\n",
        "        if mask_pos and target_tokens[i]:  # Ensure <mask> is found and target word is valid\n",
        "            for j, token_id in enumerate(target_tokens[i]):  # Assign all subword tokens\n",
        "                if mask_pos[0] + j < len(model_inputs[\"labels\"][i]):  # Avoid index errors\n",
        "                    model_inputs[\"labels\"][i][mask_pos[0] + j] = token_id\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "zY7epkJEKTmg"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verify structure\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "hdB5m8srKlUB",
        "outputId": "ef50e1ea-df89-4ac3-fbd6-23e191d26663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "84215528a2ba46dfaac1c55383bea363",
            "27d19fb9255c4583b2b1de84a37c36c2",
            "ef8e5e4e8edc41358387d8aa158f274c",
            "905b0f37cc73439e908529443bcc6e8f",
            "21b04cce8c8f42e6a15c875079158bee",
            "1769bf22a5464822bf24f405ded5c8fa",
            "16d350da50bd49b59bdfd7910e080cd8",
            "8c80abc8dcf44ead85b9a21262cd58cd",
            "bd695e19f29142d290152586082fe50a",
            "6a4ec51c39164843ab923333e116a1ba",
            "4073116836a34c808e535cdea9bdf82e"
          ]
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/492 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84215528a2ba46dfaac1c55383bea363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['word_en', 'word_fr', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 492\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample processed example\n",
        "example = tokenized_dataset[0]\n",
        "\n",
        "# Decode back to text to verify tokenization\n",
        "decoded_input = tokenizer.decode(example[\"input_ids\"])\n",
        "print(\"Tokenized Input:\", decoded_input)\n",
        "print(\"Labels (Token IDs):\", example[\"labels\"])\n",
        "\n",
        "# Decode the tokenized labels to check if they correctly represent the French word\n",
        "decoded_label_tokens = tokenizer.convert_ids_to_tokens([id for id in example[\"labels\"] if id != -100])\n",
        "print(\"Decoded Label Tokens:\", decoded_label_tokens)"
      ],
      "metadata": {
        "id": "VMWEfMmcKuvA",
        "outputId": "76f70354-15da-4e49-a46c-6dc6a18d4961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Input: <s> In English, the word is abandon. En Français, le mot est<mask> .</s><pad>\n",
            "Labels (Token IDs): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32997, -100, -100, -100, -100]\n",
            "Decoded Label Tokens: ['▁abandon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Shuffle the dataset\n",
        "tokenized_dataset = tokenized_dataset.shuffle(seed=42)\n",
        "\n",
        "# Compute split sizes\n",
        "total_size = len(tokenized_dataset)\n",
        "train_size = int(0.7 * total_size)      # 70% training\n",
        "val_size = int(0.2 * total_size)        # 20% validation\n",
        "test_size = total_size - (train_size + val_size)        # 10% test\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = tokenized_dataset.select(range(train_size))\n",
        "val_dataset = tokenized_dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = tokenized_dataset.select(range(train_size + val_size, total_size))\n",
        "\n",
        "# Verify splits\n",
        "print(f\"Total samples: {total_size}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "jU3LedPpLF3D",
        "outputId": "f140e652-d416-4ab0-8d7e-3c41cf61cf76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 492\n",
            "Training samples: 344\n",
            "Validation samples: 98\n",
            "Test samples: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Pretrained Model ##"
      ],
      "metadata": {
        "id": "RSkJL2_U2WM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_mask_accuracy(model, test_dataset, tokenizer, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of a masked language model on a cognate dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The pretrained or fine-tuned masked language model.\n",
        "        test_dataset: Hugging Face tokenized dataset with masked inputs.\n",
        "        tokenizer: The tokenizer corresponding to the model.\n",
        "        top_k (int): Number of top predictions to consider for accuracy.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model on the dataset.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(test_dataset)\n",
        "\n",
        "    for i in range(total_samples):\n",
        "        # Get tokenized input and expected labels\n",
        "        example = test_dataset[i]\n",
        "        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
        "        labels = example[\"labels\"]  # Token IDs for masked French word(s)\n",
        "\n",
        "        # Find the <mask> token index\n",
        "        mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        if len(mask_token_index[0]) == 0:  # If no <mask> token is found\n",
        "            print(f\"Error: No {tokenizer.mask_token} token found in instance {i+1}\")\n",
        "            continue\n",
        "\n",
        "        mask_token_index = mask_token_index[1]  # Get index positions of <mask> token\n",
        "\n",
        "        # Forward pass through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "        logits = outputs.logits  # Prediction scores for each token in vocabulary\n",
        "\n",
        "        # Get top-k predictions for each masked token\n",
        "        mask_token_logits = logits[0, mask_token_index, :]\n",
        "        top_k_tokens = torch.topk(mask_token_logits, k=top_k, dim=-1).indices.tolist()\n",
        "\n",
        "        # Decode predictions into words\n",
        "        predicted_words = [[tokenizer.decode([token]).strip() for token in top_k] for top_k in top_k_tokens]\n",
        "\n",
        "        # Decode the expected French word(s)\n",
        "        expected_words = tokenizer.convert_ids_to_tokens([id for id in labels if id != -100])\n",
        "\n",
        "        # Log predictions\n",
        "        print(f\"Instance {i+1}:\")\n",
        "        print(f\"    Tokenized Input: {tokenizer.decode(example['input_ids'])}\")\n",
        "        print(f\"    Expected French word(s): {expected_words}\")\n",
        "        print(f\"    Predicted MASK words (Top-{top_k}): {predicted_words}\\n\")\n",
        "\n",
        "        # Check if all expected subword tokens are predicted in top-k\n",
        "        if all(any(subword in pred_list for pred_list in predicted_words) for subword in expected_words):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Evaluate the accuracy of pretrained model\n",
        "accuracy = evaluate_mask_accuracy(pretrained_model, test_dataset, tokenizer, top_k=5)\n",
        "print(f\"Accuracy of the model: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imi10wc3x9E-",
        "outputId": "f4a4f258-4537-46d5-ae0a-26fa8303d105"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instance 1:\n",
            "    Tokenized Input: <s> The English word is mystery. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁my', 'st', 'ére']\n",
            "    Predicted MASK words (Top-5): [['mystery', 'mysterie', 'misterio', 'misteri', 'science']]\n",
            "\n",
            "Instance 2:\n",
            "    Tokenized Input: <s> The English word is rhythm. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁rythme']\n",
            "    Predicted MASK words (Top-5): [['time', 'rytm', 'motion', 'movement', 'ритм']]\n",
            "\n",
            "Instance 3:\n",
            "    Tokenized Input: <s> The English word is baron. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁bar', 'on']\n",
            "    Predicted MASK words (Top-5): [['bar', 'river', 'man', 'king', 'garden']]\n",
            "\n",
            "Instance 4:\n",
            "    Tokenized Input: <s> The English word is difference. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁différence']\n",
            "    Predicted MASK words (Top-5): [['difference', 'different', 'similar', 'change', 'opposite']]\n",
            "\n",
            "Instance 5:\n",
            "    Tokenized Input: <s> The English word is biography. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁bi', 'ographie']\n",
            "    Predicted MASK words (Top-5): [['language', 'history', 'science', 'geografi', 'English']]\n",
            "\n",
            "Instance 6:\n",
            "    Tokenized Input: <s> The English word is conductor. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁conduct', 'eur']\n",
            "    Predicted MASK words (Top-5): [['conductor', 'direction', 'condutor', 'conduct', 'medium']]\n",
            "\n",
            "Instance 7:\n",
            "    Tokenized Input: <s> The English word is element. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁élément']\n",
            "    Predicted MASK words (Top-5): [['element', 'Element', 'elements', 'atom', 'component']]\n",
            "\n",
            "Instance 8:\n",
            "    Tokenized Input: <s> The English word is admirable. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁admira', 'ble']\n",
            "    Predicted MASK words (Top-5): [['excellent', 'great', 'wonderful', 'perfect', 'beautiful']]\n",
            "\n",
            "Instance 9:\n",
            "    Tokenized Input: <s> The English word is emotion. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁', 'émotion']\n",
            "    Predicted MASK words (Top-5): [['emotion', 'feeling', 'emotional', 'emo', 'sentiment']]\n",
            "\n",
            "Instance 10:\n",
            "    Tokenized Input: <s> The English word is question. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁question']\n",
            "    Predicted MASK words (Top-5): [['question', 'answer', 'word', 'questions', 'Question']]\n",
            "\n",
            "Instance 11:\n",
            "    Tokenized Input: <s> The English word is liberty. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁liberté']\n",
            "    Predicted MASK words (Top-5): [['freedom', 'liber', 'free', 'libre', 'liberté']]\n",
            "\n",
            "Instance 12:\n",
            "    Tokenized Input: <s> The English word is destruction. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁destruction']\n",
            "    Predicted MASK words (Top-5): [['destruction', 'death', 'destroy', 'dead', 'damage']]\n",
            "\n",
            "Instance 13:\n",
            "    Tokenized Input: <s> The English word is international. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁international']\n",
            "    Predicted MASK words (Top-5): [['English', 'international', 'International', 'english', 'latin']]\n",
            "\n",
            "Instance 14:\n",
            "    Tokenized Input: <s> The English word is composition. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁composition']\n",
            "    Predicted MASK words (Top-5): [['composition', 'compose', 'structure', 'language', 'word']]\n",
            "\n",
            "Instance 15:\n",
            "    Tokenized Input: <s> The English word is quintal. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁quinta', 'l']\n",
            "    Predicted MASK words (Top-5): [['quinta', 'lettre', 'quart', 'kvadrat', 'double']]\n",
            "\n",
            "Instance 16:\n",
            "    Tokenized Input: <s> The English word is racism. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁rac', 'isme']\n",
            "    Predicted MASK words (Top-5): [['race', 'violence', 'religion', 'freedom', 'socialism']]\n",
            "\n",
            "Instance 17:\n",
            "    Tokenized Input: <s> The English word is frigate. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁f', 'ré', 'gate']\n",
            "    Predicted MASK words (Top-5): [['French', 'verb', 'English', 'language', 'lettre']]\n",
            "\n",
            "Instance 18:\n",
            "    Tokenized Input: <s> The English word is aggression. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁aggressi', 'on']\n",
            "    Predicted MASK words (Top-5): [['violence', 'attack', 'fighting', 'war', 'fear']]\n",
            "\n",
            "Instance 19:\n",
            "    Tokenized Input: <s> The English word is humanity. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁humanit', 'é']\n",
            "    Predicted MASK words (Top-5): [['life', 'science', 'language', 'human', 'nature']]\n",
            "\n",
            "Instance 20:\n",
            "    Tokenized Input: <s> The English word is oratory. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁ora', 'toire']\n",
            "    Predicted MASK words (Top-5): [['language', 'education', 'school', 'university', 'science']]\n",
            "\n",
            "Instance 21:\n",
            "    Tokenized Input: <s> The English word is parchment. In French, it is<mask> .</s><pad><pad>\n",
            "    Expected French word(s): ['▁par', 'che', 'min']\n",
            "    Predicted MASK words (Top-5): [['room', 'house', 'building', 'property', 'appartement']]\n",
            "\n",
            "Instance 22:\n",
            "    Tokenized Input: <s> The English word is hotel. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁hôtel']\n",
            "    Predicted MASK words (Top-5): [['hotel', 'house', 'room', 'Hotel', 'restaurant']]\n",
            "\n",
            "Instance 23:\n",
            "    Tokenized Input: <s> The English word is music. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁musique']\n",
            "    Predicted MASK words (Top-5): [['music', 'language', 'Music', 'art', 'word']]\n",
            "\n",
            "Instance 24:\n",
            "    Tokenized Input: <s> The English word is ceramic. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁cé', 'ram', 'ique']\n",
            "    Predicted MASK words (Top-5): [['ceramic', 'wood', 'glass', 'keramik', 'metal']]\n",
            "\n",
            "Instance 25:\n",
            "    Tokenized Input: <s> The English word is skeleton. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁', 'sque', 'lette']\n",
            "    Predicted MASK words (Top-5): [['skelet', 'pyramid', 'joint', 'structure', 'body']]\n",
            "\n",
            "Instance 26:\n",
            "    Tokenized Input: <s> The English word is ganglion. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁gang', 'lion']\n",
            "    Predicted MASK words (Top-5): [['division', 'fish', 'gang', 'ball', 'egg']]\n",
            "\n",
            "Instance 27:\n",
            "    Tokenized Input: <s> The English word is absolution. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁absolut', 'ion']\n",
            "    Predicted MASK words (Top-5): [['absolute', 'union', 'condition', 'absolut', 'perfection']]\n",
            "\n",
            "Instance 28:\n",
            "    Tokenized Input: <s> The English word is objective. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁', 'objectif']\n",
            "    Predicted MASK words (Top-5): [['objective', 'good', 'impossible', 'subject', 'simple']]\n",
            "\n",
            "Instance 29:\n",
            "    Tokenized Input: <s> The English word is illegal. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁', 'illé', 'gal']\n",
            "    Predicted MASK words (Top-5): [['illegal', 'legal', 'English', 'wrong', 'free']]\n",
            "\n",
            "Instance 30:\n",
            "    Tokenized Input: <s> The English word is context. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁contexte']\n",
            "    Predicted MASK words (Top-5): [['context', 'meaning', 'verb', 'language', 'kontekst']]\n",
            "\n",
            "Instance 31:\n",
            "    Tokenized Input: <s> The English word is angel. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁ange']\n",
            "    Predicted MASK words (Top-5): [['angel', 'Angel', 'Deus', 'a', 'engel']]\n",
            "\n",
            "Instance 32:\n",
            "    Tokenized Input: <s> The English word is offensive. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁offen', 'sif']\n",
            "    Predicted MASK words (Top-5): [['offensiv', 'funny', 'strong', 'vulgar', 'good']]\n",
            "\n",
            "Instance 33:\n",
            "    Tokenized Input: <s> The English word is detail. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁détail']\n",
            "    Predicted MASK words (Top-5): [['detail', 'details', 'Detail', 'detailed', 'description']]\n",
            "\n",
            "Instance 34:\n",
            "    Tokenized Input: <s> The English word is gorilla. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁gor', 'ille']\n",
            "    Predicted MASK words (Top-5): [['gor', 'gora', 'river', 'mountain', 'mare']]\n",
            "\n",
            "Instance 35:\n",
            "    Tokenized Input: <s> The English word is adversary. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁advers', 'aire']\n",
            "    Predicted MASK words (Top-5): [['opposition', 'opposite', 'lawan', 'rival', 'relation']]\n",
            "\n",
            "Instance 36:\n",
            "    Tokenized Input: <s> The English word is dozen. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁do', 'uza', 'ine']\n",
            "    Predicted MASK words (Top-5): [['eight', 'seven', 'five', 'done', 'two']]\n",
            "\n",
            "Instance 37:\n",
            "    Tokenized Input: <s> The English word is turtle. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁tortu', 'e']\n",
            "    Predicted MASK words (Top-5): [['tall', 'tree', 'tur', 'string', ':']]\n",
            "\n",
            "Instance 38:\n",
            "    Tokenized Input: <s> The English word is reaction. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁réaction']\n",
            "    Predicted MASK words (Top-5): [['reaction', 'response', 'action', 'react', 'effect']]\n",
            "\n",
            "Instance 39:\n",
            "    Tokenized Input: <s> The English word is narration. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁narra', 'tion']\n",
            "    Predicted MASK words (Top-5): [['story', 'description', 'language', 'telling', 'dialogue']]\n",
            "\n",
            "Instance 40:\n",
            "    Tokenized Input: <s> The English word is femur. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁fé', 'mur']\n",
            "    Predicted MASK words (Top-5): [['sex', 'woman', 'femme', 'penis', 'wife']]\n",
            "\n",
            "Instance 41:\n",
            "    Tokenized Input: <s> The English word is pirate. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁pirat', 'e']\n",
            "    Predicted MASK words (Top-5): [['pirat', 'fish', 'pipe', 'boat', 'ship']]\n",
            "\n",
            "Instance 42:\n",
            "    Tokenized Input: <s> The English word is kepi. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁kép', 'i']\n",
            "    Predicted MASK words (Top-5): [['pi', 'soup', 'pipe', 'api', 'keel']]\n",
            "\n",
            "Instance 43:\n",
            "    Tokenized Input: <s> The English word is phosphate. In French, it is<mask> .</s><pad>\n",
            "    Expected French word(s): ['▁', 'phos', 'pha', 'te']\n",
            "    Predicted MASK words (Top-5): [['acid', 'water', 'oxygen', 'magnesium', ':']]\n",
            "\n",
            "Instance 44:\n",
            "    Tokenized Input: <s> The English word is accessory. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁accesso', 'ire']\n",
            "    Predicted MASK words (Top-5): [['extension', 'accent', 'complement', 'synonym', 'something']]\n",
            "\n",
            "Instance 45:\n",
            "    Tokenized Input: <s> The English word is diagonal. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁diagonal']\n",
            "    Predicted MASK words (Top-5): [['diagonal', 'vertical', 'straight', 'horizontal', 'flat']]\n",
            "\n",
            "Instance 46:\n",
            "    Tokenized Input: <s> The English word is fusion. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁fusion']\n",
            "    Predicted MASK words (Top-5): [['fusion', 'verb', 'union', 'word', 'Fusion']]\n",
            "\n",
            "Instance 47:\n",
            "    Tokenized Input: <s> The English word is quarter. In French, it is<mask> .</s><pad><pad><pad><pad>\n",
            "    Expected French word(s): ['▁quartier']\n",
            "    Predicted MASK words (Top-5): [['quarter', 'half', 'four', 'two', 'quart']]\n",
            "\n",
            "Instance 48:\n",
            "    Tokenized Input: <s> The English word is abolition. In French, it is<mask> .</s><pad><pad>\n",
            "    Expected French word(s): ['▁a', 'boli', 'tion']\n",
            "    Predicted MASK words (Top-5): [['union', 'freedom', 'division', 'revolution', 'verb']]\n",
            "\n",
            "Instance 49:\n",
            "    Tokenized Input: <s> The English word is contradiction. In French, it is<mask> .</s><pad><pad>\n",
            "    Expected French word(s): ['▁contra', 'di', 'ction']\n",
            "    Predicted MASK words (Top-5): [['relation', 'opposition', 'plural', 'union', 'synonym']]\n",
            "\n",
            "Instance 50:\n",
            "    Tokenized Input: <s> The English word is faction. In French, it is<mask> .</s><pad><pad><pad>\n",
            "    Expected French word(s): ['▁fac', 'tion']\n",
            "    Predicted MASK words (Top-5): [['division', 'union', 'section', 'formation', 'organization']]\n",
            "\n",
            "Accuracy of the model: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Model ##"
      ],
      "metadata": {
        "id": "8BGx9YByADkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load accuracy metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define metric computation function\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy during validation by ignoring padding tokens.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Flatten predictions and labels (remove -100 labels)\n",
        "    flattened_predictions = []\n",
        "    flattened_labels = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        for p, l in zip(pred, label):\n",
        "            if l != -100:       # Ignore padding token labels\n",
        "                flattened_predictions.append(p)\n",
        "                flattened_labels.append(l)\n",
        "\n",
        "    return accuracy.compute(predictions=flattened_predictions, references=flattened_labels)"
      ],
      "metadata": {
        "id": "_m7n4ywIAAr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define MLM Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "HszGY90B8hMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Define training arguments\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/cognate_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=6,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.03,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=pretrained_model,\n",
        "    args=arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,       # NB: change for test\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bWWSL8L2Arwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify dataset format\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a DataLoader for debugging\n",
        "debug_loader = DataLoader(train_dataset, batch_size=8)\n",
        "\n",
        "# Get a batch\n",
        "batch = next(iter(debug_loader))\n",
        "print(batch)\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
        "print(f\"Attention Mask shape: {batch['attention_mask'].shape}\")"
      ],
      "metadata": {
        "id": "o1lfC--iOdvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "mppzuBT_BLcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/cognate_trainer_best_model\"\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "foKaxwLvBRK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load finetuned model\n",
        "finetuned_model = AutoModelForMaskedLM.from_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "uH0FauszSrrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Finetuned Model ##"
      ],
      "metadata": {
        "id": "b7OyuoZQUpTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of finetuned model\n",
        "accuracy = evaluate_mask_accuracy(finetuned_model, test_data, tokenizer, top_k=5)\n",
        "print(f\"Accuracy of the model: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "MkpzXzLxTrbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization ##"
      ],
      "metadata": {
        "id": "j1G1KwYyU02w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pretrained_model.config.output_hidden_states = True\n",
        "finetuned_model.config.output_hidden_states = True\n",
        "\n",
        "def visualize_embeddings_2D(model, tokenizer, test_data, method=\"pca\"):\n",
        "    \"\"\"\n",
        "    Visualizes word embeddings from the model in 2D space using PCA or t-SNE.\n",
        "\n",
        "    Args:\n",
        "        model: The masked language model (pretrained or fine-tuned).\n",
        "        tokenizer: Tokenizer corresponding to the model.\n",
        "        test_data (list of dict): List of cognate pairs from the test split.\n",
        "        method (str): Dimensionality reduction method (\"pca\" or \"tsne\").\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    embeddings = []\n",
        "\n",
        "    # Extract embeddings\n",
        "    for pair in test_data:\n",
        "        english_word, french_word = list(pair.items())[0]\n",
        "        for word in [english_word, french_word]:\n",
        "            tokens = tokenizer(word, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=8)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**tokens)\n",
        "                hidden_states = outputs.hidden_states[-1]       # Get the last hidden layer\n",
        "                word_embedding = hidden_states.mean(dim=1).squeeze().numpy()        # Mean pooling over tokens\n",
        "                embeddings.append(word_embedding)\n",
        "                words.append(word)\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Reduce dimensions\n",
        "    if method == \"pca\":\n",
        "        reducer = PCA(n_components=2)\n",
        "    elif method == \"tsne\":\n",
        "        from sklearn.manifold import TSNE\n",
        "        reducer = TSNE(n_components=2, perplexity=10, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'pca' or 'tsne'.\")\n",
        "\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Plot embeddings\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, c=np.arange(len(words)) % 2, cmap=\"coolwarm\")\n",
        "\n",
        "    # Annotate points with words\n",
        "    for i, word in enumerate(words):\n",
        "        plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word, fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "    plt.title(f\"2D Projection of Word Embeddings for ({method.upper()})\")\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Call function for both models using PCA\n",
        "visualize_embeddings_2D(pretrained_model, tokenizer, test_data, method=\"pca\")\n",
        "visualize_embeddings_2D(finetuned_model, tokenizer, test_data, method=\"pca\")"
      ],
      "metadata": {
        "id": "VKF4-FGwU0Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Section ##"
      ],
      "metadata": {
        "id": "BCCXZ4BhfEPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "word = \"variante\"\n",
        "\n",
        "tokens = tokenizer(word, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    pre_emb = pretrained_model(**tokens).hidden_states[-1].mean(dim=1).squeeze().detach().numpy()\n",
        "    fine_emb = finetuned_model(**tokens).hidden_states[-1].mean(dim=1).squeeze().detach().numpy()\n",
        "\n",
        "similarity = 1 - cosine(pre_emb, fine_emb)\n",
        "print(f\"Cosine similarity between pretrained and fine-tuned embeddings for '{word}': {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "YTne_eNvdbzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = tokenizer(\"The English word is risk. In French, it is <mask>.\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    pretrained_preds = pretrained_model(**sample_input).logits\n",
        "    finetuned_preds = finetuned_model(**sample_input).logits\n",
        "\n",
        "# Compute difference in logits\n",
        "logit_diff = (pretrained_preds - finetuned_preds).abs().sum().item()\n",
        "print(\"Logit difference before & after fine-tuning:\", logit_diff)"
      ],
      "metadata": {
        "id": "D9S6qNxcyhVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total training steps: {arguments.max_steps}\")"
      ],
      "metadata": {
        "id": "bBZ6pI0MzHFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}