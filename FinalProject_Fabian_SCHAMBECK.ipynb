{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "927b29039c9e4f44aee9fed9400280b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3dd418a2d6054076a735ea932a83c6d1",
              "IPY_MODEL_8433859ae40a4349907f74568ace74c5",
              "IPY_MODEL_cf4ad76ef3c14c479231fab33672a254"
            ],
            "layout": "IPY_MODEL_f864266720554bb19d5c86dfcab3a98e"
          }
        },
        "3dd418a2d6054076a735ea932a83c6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4409608babe4e6da879b90dbcb62802",
            "placeholder": "​",
            "style": "IPY_MODEL_dfc03aa4670a4de1b8f0b3ae58735796",
            "value": "Map: 100%"
          }
        },
        "8433859ae40a4349907f74568ace74c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df8ecb4d5ebc4764a800b66059497510",
            "max": 492,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1421983c25914a879dbfd53e1f40fe7a",
            "value": 492
          }
        },
        "cf4ad76ef3c14c479231fab33672a254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4546c7cc166b4daa8102b9ad11b62c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_d0a24c82ea624e938b449e1ce5baf5c7",
            "value": " 492/492 [00:00&lt;00:00, 2059.41 examples/s]"
          }
        },
        "f864266720554bb19d5c86dfcab3a98e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4409608babe4e6da879b90dbcb62802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc03aa4670a4de1b8f0b3ae58735796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df8ecb4d5ebc4764a800b66059497510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1421983c25914a879dbfd53e1f40fe7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4546c7cc166b4daa8102b9ad11b62c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a24c82ea624e938b449e1ce5baf5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fubotz/ICL_2024W/blob/main/FinalProject_Fabian_SCHAMBECK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ICL Final Project: Finetuning a Pretrained Multilingual Model for Cognate Detection\n",
        "\n",
        "Model: xlm-roberta-base\n",
        "\n",
        "Dataset: custom dataset containing en-fr cognates (Frossard et al.)\n",
        "\n",
        "Method: MASK approach"
      ],
      "metadata": {
        "id": "N4_fSCGEAFZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q transformers\n",
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "v1II96WS0W56"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YwUn733bt7lE",
        "outputId": "5a46772a-eace-4b0c-f948-7cae85bad527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset ##"
      ],
      "metadata": {
        "id": "5HQHtDU0_JW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/fubotz/ICL_2024W/refs/heads/main/word_pairs.json        # dataset taken from Frossard et al."
      ],
      "metadata": {
        "id": "kcC8-tRZ-5yb",
        "outputId": "d438192b-6842-46bd-a1cb-40200a7cf96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-05 21:05:22--  https://raw.githubusercontent.com/fubotz/ICL_2024W/refs/heads/main/word_pairs.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23242 (23K) [text/plain]\n",
            "Saving to: ‘word_pairs.json.1’\n",
            "\n",
            "word_pairs.json.1   100%[===================>]  22.70K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-02-05 21:05:23 (2.75 MB/s) - ‘word_pairs.json.1’ saved [23242/23242]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"word_pairs.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "print(dataset[:10])"
      ],
      "metadata": {
        "id": "JTOGfoFS_MaX",
        "outputId": "4e0e7514-7be9-41dd-997e-0769e5e6ec30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'abandon': 'abandon'}, {'abbe': 'abbé'}, {'abdomen': 'abdomen'}, {'abdominal': 'abdominal'}, {'aberration': 'aberration'}, {'abolition': 'abolition'}, {'abominable': 'abominable'}, {'absence': 'absence'}, {'absolute': 'absolu'}, {'absolution': 'absolution'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert the dataset to a dictionary format with separate lists for English and French words\n",
        "dataset_dict = {\n",
        "    \"word_en\": [list(pair.keys())[0] for pair in dataset],      # Extract English words\n",
        "    \"word_fr\": [list(pair.values())[0] for pair in dataset]     # Extract French words\n",
        "}\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Verify structure\n",
        "print(dataset, \"\\n\")\n",
        "print(dataset[:10])"
      ],
      "metadata": {
        "id": "qmP-UoqdJmrn",
        "outputId": "ba814b04-e6ce-4299-b4f1-17e64ae2a4bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['word_en', 'word_fr'],\n",
            "    num_rows: 492\n",
            "}) \n",
            "\n",
            "{'word_en': ['abandon', 'abbe', 'abdomen', 'abdominal', 'aberration', 'abolition', 'abominable', 'absence', 'absolute', 'absolution'], 'word_fr': ['abandon', 'abbé', 'abdomen', 'abdominal', 'aberration', 'abolition', 'abominable', 'absence', 'absolu', 'absolution']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model ##"
      ],
      "metadata": {
        "id": "qyBJxKpF-9wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Model for <mask> approach\n",
        "pretrained_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Freeze and unfreeze x encoder layers\n",
        "for param in pretrained_model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in pretrained_model.base_model.encoder.layer[-5:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "vQlmoRnEQykz",
        "outputId": "fa90115c-1c9f-4a77-9117-664740011970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Dataset ##"
      ],
      "metadata": {
        "id": "51DQW-EYKURY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes input words, replaces the French word with <mask>,\n",
        "    and assigns ALL subword tokens (BytePair Encoding) of the correct target\n",
        "    word as labels.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A batch of English-French cognate pairs in dictionary format:\n",
        "                         {\"word_en\": [...], \"word_fr\": [...]}\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - input_ids: Tokenized sentences with <mask>\n",
        "            - attention_mask: Mask indicating valid tokens\n",
        "            - labels: Correct token IDs for the French word at the <mask> position\n",
        "    \"\"\"\n",
        "    # Construct masked input sentences\n",
        "    masked_sentences = [\n",
        "        f\"In English, the word is {word_en}. En Français, le mot est {tokenizer.mask_token}.\"\n",
        "        for word_en in examples[\"word_en\"]\n",
        "    ]\n",
        "\n",
        "    # Tokenize input sentences\n",
        "    model_inputs = tokenizer(masked_sentences, max_length=20, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Find <mask> token indices\n",
        "    mask_indices = [\n",
        "        [i for i, token_id in enumerate(input_ids) if token_id == tokenizer.mask_token_id]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    # Tokenize target words (French cognates) WITHOUT special tokens\n",
        "    target_tokens = tokenizer(examples[\"word_fr\"], add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    # Initialize label tensor with -100 (ignored positions)\n",
        "    model_inputs[\"labels\"] = [[-100] * len(input_ids) for input_ids in model_inputs[\"input_ids\"]]\n",
        "\n",
        "    # Assign correct token IDs at the <mask> position\n",
        "    for i, mask_pos in enumerate(mask_indices):\n",
        "        if mask_pos and target_tokens[i]:  # Ensure <mask> is found and target word is valid\n",
        "            for j, token_id in enumerate(target_tokens[i]):  # Assign all subword tokens\n",
        "                if mask_pos[0] + j < len(model_inputs[\"labels\"][i]):  # Avoid index errors\n",
        "                    model_inputs[\"labels\"][i][mask_pos[0] + j] = token_id\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "zY7epkJEKTmg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verify structure\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "hdB5m8srKlUB",
        "outputId": "c5c03294-019f-4eef-8b4d-87aeb9d8fe1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "927b29039c9e4f44aee9fed9400280b4",
            "3dd418a2d6054076a735ea932a83c6d1",
            "8433859ae40a4349907f74568ace74c5",
            "cf4ad76ef3c14c479231fab33672a254",
            "f864266720554bb19d5c86dfcab3a98e",
            "c4409608babe4e6da879b90dbcb62802",
            "dfc03aa4670a4de1b8f0b3ae58735796",
            "df8ecb4d5ebc4764a800b66059497510",
            "1421983c25914a879dbfd53e1f40fe7a",
            "4546c7cc166b4daa8102b9ad11b62c8e",
            "d0a24c82ea624e938b449e1ce5baf5c7"
          ]
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/492 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "927b29039c9e4f44aee9fed9400280b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['word_en', 'word_fr', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 492\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Shuffle the dataset\n",
        "tokenized_dataset = tokenized_dataset.shuffle(seed=24)\n",
        "\n",
        "# Compute split sizes\n",
        "total_size = len(tokenized_dataset)\n",
        "train_size = int(0.7 * total_size)      # 70% training\n",
        "val_size = int(0.2 * total_size)        # 20% validation\n",
        "test_size = total_size - (train_size + val_size)        # 10% test\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset = tokenized_dataset.select(range(train_size))\n",
        "val_dataset = tokenized_dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = tokenized_dataset.select(range(train_size + val_size, total_size))\n",
        "\n",
        "# Verify splits\n",
        "print(f\"Total samples: {total_size}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "jU3LedPpLF3D",
        "outputId": "9ab54ad6-429a-42e0-cc34-4766ac91cfe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 492\n",
            "Training samples: 344\n",
            "Validation samples: 98\n",
            "Test samples: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample processed example\n",
        "example = tokenized_dataset[0]\n",
        "\n",
        "# Decode back to text to verify tokenization\n",
        "decoded_input = tokenizer.decode(example[\"input_ids\"])\n",
        "print(\"Tokenized Input:\", decoded_input)\n",
        "print(\"Labels (Token IDs):\", example[\"labels\"])\n",
        "\n",
        "# Decode the tokenized labels to check if they correctly represent the French word\n",
        "decoded_label_tokens = tokenizer.convert_ids_to_tokens([id for id in example[\"labels\"] if id != -100])\n",
        "print(\"Decoded Label Tokens:\", decoded_label_tokens)"
      ],
      "metadata": {
        "id": "o5PpfR4ZaIUR",
        "outputId": "cc0eba93-2e7a-4905-e22c-6444008e7c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Input: <s> In English, the word is bis. En Français, le mot est<mask> .</s><pad>\n",
            "Labels (Token IDs): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2464, -100, -100, -100, -100]\n",
            "Decoded Label Tokens: ['▁bis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Pretrained Model ##"
      ],
      "metadata": {
        "id": "RSkJL2_U2WM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation function\n",
        "def evaluate_mask_accuracy(model, test_dataset, tokenizer, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of a masked language model on a cognate dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The pretrained or fine-tuned masked language model.\n",
        "        test_dataset: Hugging Face tokenized dataset with masked inputs.\n",
        "        tokenizer: The tokenizer corresponding to the model.\n",
        "        top_k (int): Number of top predictions to consider for accuracy.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model on the dataset.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(test_dataset)\n",
        "\n",
        "    for i in range(total_samples):\n",
        "        # Get tokenized input and expected labels\n",
        "        example = test_dataset[i]\n",
        "        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
        "        labels = example[\"labels\"]  # Token IDs for masked French word(s)\n",
        "        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0)  # Ensure padding is properly masked\n",
        "\n",
        "        # Find the <mask> token index\n",
        "        mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        if len(mask_token_index[0]) == 0:  # If no <mask> token is found\n",
        "            print(f\"Error: No {tokenizer.mask_token} token found in instance {i+1}\")\n",
        "            continue\n",
        "\n",
        "        mask_token_index = mask_token_index[1]  # Get index positions of <mask> token\n",
        "\n",
        "        # Forward pass through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "        logits = outputs.logits  # Prediction scores for each token in vocabulary\n",
        "\n",
        "        # Get top-k predictions for each masked token\n",
        "        mask_token_logits = logits[0, mask_token_index, :]\n",
        "        top_k_tokens = torch.topk(mask_token_logits, k=top_k, dim=-1).indices.tolist()\n",
        "\n",
        "        # Decode predictions into words\n",
        "        predicted_words = [[tokenizer.decode([token]).strip() for token in top_k] for top_k in top_k_tokens]\n",
        "\n",
        "        # Decode and format the expected French word\n",
        "        expected_tokens = [id for id in labels if id != -100]  # Remove ignored tokens\n",
        "        expected_word_pieces = tokenizer.convert_ids_to_tokens(expected_tokens)\n",
        "        expected_word = \"\".join([piece.lstrip(\"▁\") for piece in expected_word_pieces])  # Concatenate and remove '▁'\n",
        "\n",
        "\n",
        "        # Log predictions\n",
        "        print(f\"Instance {i+1}:\")\n",
        "        print(f\"    Tokenized Input: {tokenizer.decode(example['input_ids'])}\")\n",
        "        print(f\"    Expected French word: {expected_word}\")\n",
        "        print(f\"    Predicted MASK words (Top-{top_k}): {predicted_words}\\n\")\n",
        "\n",
        "        # Check if all expected subword tokens are predicted in top-k\n",
        "        if any(expected_word in pred_list for pred_list in predicted_words):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Evaluate the accuracy of pretrained model\n",
        "accuracy = evaluate_mask_accuracy(pretrained_model, test_dataset, tokenizer, top_k=5)\n",
        "print(f\"Accuracy of the model: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imi10wc3x9E-",
        "outputId": "2ebb28c9-7d92-4fd0-c5b5-5f580c46332a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instance 1:\n",
            "    Tokenized Input: <s> In English, the word is habitation. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: habitation\n",
            "    Predicted MASK words (Top-5): [['logement', 'maison', ':', ':', 'habitat']]\n",
            "\n",
            "Instance 2:\n",
            "    Tokenized Input: <s> In English, the word is journalist. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: journaliste\n",
            "    Predicted MASK words (Top-5): [['reporter', 'journalist', 'avocat', 'directeur', 'magistrat']]\n",
            "\n",
            "Instance 3:\n",
            "    Tokenized Input: <s> In English, the word is longitude. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: longitude\n",
            "    Predicted MASK words (Top-5): [['longitud', 'de', 'long', 'longueur', 'angle']]\n",
            "\n",
            "Instance 4:\n",
            "    Tokenized Input: <s> In English, the word is ceremony. En Français, le mot est<mask> </s>\n",
            "    Expected French word: cérémonie\n",
            "    Predicted MASK words (Top-5): [['cérémonie', ':', ':', '.', '']]\n",
            "\n",
            "Instance 5:\n",
            "    Tokenized Input: <s> In English, the word is aberration. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: aberration\n",
            "    Predicted MASK words (Top-5): [[':', 'expression', ':', 'un', 'a']]\n",
            "\n",
            "Instance 6:\n",
            "    Tokenized Input: <s> In English, the word is temptation. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: tentation\n",
            "    Predicted MASK words (Top-5): [[':', ':', 'pour', 'de', 'temp']]\n",
            "\n",
            "Instance 7:\n",
            "    Tokenized Input: <s> In English, the word is enumeration. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: énumération\n",
            "    Predicted MASK words (Top-5): [['description', 'addition', 'extension', 'division', 'observation']]\n",
            "\n",
            "Instance 8:\n",
            "    Tokenized Input: <s> In English, the word is adolescent. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: adolescent\n",
            "    Predicted MASK words (Top-5): [['adolescents', 'jeune', 'adolescente', 'teen', 'mature']]\n",
            "\n",
            "Instance 9:\n",
            "    Tokenized Input: <s> In English, the word is rational. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: rationnel\n",
            "    Predicted MASK words (Top-5): [['rational', 'logique', 'raison', 'prudent', 'intelligent']]\n",
            "\n",
            "Instance 10:\n",
            "    Tokenized Input: <s> In English, the word is articulation. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: articulation\n",
            "    Predicted MASK words (Top-5): [['structure', 'expression', 'extension', 'articula', 'relation']]\n",
            "\n",
            "Instance 11:\n",
            "    Tokenized Input: <s> In English, the word is academy. En Français, le mot est<mask> </s>\n",
            "    Expected French word: académie\n",
            "    Predicted MASK words (Top-5): [['Academy', ':', ':', 'a', 'à']]\n",
            "\n",
            "Instance 12:\n",
            "    Tokenized Input: <s> In English, the word is magnificence. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: magnificence\n",
            "    Predicted MASK words (Top-5): [['excellent', 'perfection', ':', ':', 'magnific']]\n",
            "\n",
            "Instance 13:\n",
            "    Tokenized Input: <s> In English, the word is independent. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: indépendant\n",
            "    Predicted MASK words (Top-5): [['independent', 'indépendant', 'dependent', 'anglais', 'distinct']]\n",
            "\n",
            "Instance 14:\n",
            "    Tokenized Input: <s> In English, the word is female. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: femelle\n",
            "    Predicted MASK words (Top-5): [['femme', 'homme', 'femmes', 'hommes', 'mère']]\n",
            "\n",
            "Instance 15:\n",
            "    Tokenized Input: <s> In English, the word is catastrophe. En Français, le mot est<mask> </s>\n",
            "    Expected French word: catastrophe\n",
            "    Predicted MASK words (Top-5): [[':', ':', 'de', '.', 'cata']]\n",
            "\n",
            "Instance 16:\n",
            "    Tokenized Input: <s> In English, the word is sociology. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: sociologie\n",
            "    Predicted MASK words (Top-5): [['social', 'sociale', 'science', 'soci', ':']]\n",
            "\n",
            "Instance 17:\n",
            "    Tokenized Input: <s> In English, the word is chocolate. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: chocolat\n",
            "    Predicted MASK words (Top-5): [['chocolat', 'caramel', 'chocolate', 'sucre', 'crème']]\n",
            "\n",
            "Instance 18:\n",
            "    Tokenized Input: <s> In English, the word is danger. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: danger\n",
            "    Predicted MASK words (Top-5): [['danger', 'risque', 'sécurité', 'accident', 'protection']]\n",
            "\n",
            "Instance 19:\n",
            "    Tokenized Input: <s> In English, the word is noun. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: nom\n",
            "    Predicted MASK words (Top-5): [['un', 'nom', 'non', 'number', ':']]\n",
            "\n",
            "Instance 20:\n",
            "    Tokenized Input: <s> In English, the word is real. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: réel\n",
            "    Predicted MASK words (Top-5): [['réel', 'mort', 'real', 'vrai', 'écrit']]\n",
            "\n",
            "Instance 21:\n",
            "    Tokenized Input: <s> In English, the word is american. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: américain\n",
            "    Predicted MASK words (Top-5): [['anglais', 'français', 'américain', 'allemand', 'latin']]\n",
            "\n",
            "Instance 22:\n",
            "    Tokenized Input: <s> In English, the word is dictator. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: dictateur\n",
            "    Predicted MASK words (Top-5): [['dicta', ':', 'de', 'président', ':']]\n",
            "\n",
            "Instance 23:\n",
            "    Tokenized Input: <s> In English, the word is ligament. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: ligament\n",
            "    Predicted MASK words (Top-5): [['liga', 'division', ':', 'le', 'joint']]\n",
            "\n",
            "Instance 24:\n",
            "    Tokenized Input: <s> In English, the word is poem. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: poème\n",
            "    Predicted MASK words (Top-5): [['poem', 'texte', 'phrase', 'écrit', 'chanson']]\n",
            "\n",
            "Instance 25:\n",
            "    Tokenized Input: <s> In English, the word is authentic. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: authentique\n",
            "    Predicted MASK words (Top-5): [['authentic', 'français', 'anglais', 'inconnu', 'original']]\n",
            "\n",
            "Instance 26:\n",
            "    Tokenized Input: <s> In English, the word is expert. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: expert\n",
            "    Predicted MASK words (Top-5): [['expert', 'excellent', 'professionnel', 'spécialiste', 'experts']]\n",
            "\n",
            "Instance 27:\n",
            "    Tokenized Input: <s> In English, the word is ideal. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: idéal\n",
            "    Predicted MASK words (Top-5): [['ideal', 'excellent', 'parfait', 'perfect', 'impossible']]\n",
            "\n",
            "Instance 28:\n",
            "    Tokenized Input: <s> In English, the word is chrome. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: chrome\n",
            "    Predicted MASK words (Top-5): [['latin', 'français', 'anglais', 'grec', 'allemand']]\n",
            "\n",
            "Instance 29:\n",
            "    Tokenized Input: <s> In English, the word is cage. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: cage\n",
            "    Predicted MASK words (Top-5): [['ca', ':', ':', 'de', 'cache']]\n",
            "\n",
            "Instance 30:\n",
            "    Tokenized Input: <s> In English, the word is government. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: gouvernement\n",
            "    Predicted MASK words (Top-5): [['gouvernement', 'government', 'administration', 'pouvoir', 'Administration']]\n",
            "\n",
            "Instance 31:\n",
            "    Tokenized Input: <s> In English, the word is loyalty. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: loyauté\n",
            "    Predicted MASK words (Top-5): [['loyal', ':', ':', 'de', 'le']]\n",
            "\n",
            "Instance 32:\n",
            "    Tokenized Input: <s> In English, the word is phosphate. En Français, le mot est<mask></s>\n",
            "    Expected French word: phos\n",
            "    Predicted MASK words (Top-5): [['</s>', '.', ':', ':', 'acid']]\n",
            "\n",
            "Instance 33:\n",
            "    Tokenized Input: <s> In English, the word is dialect. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: dialecte\n",
            "    Predicted MASK words (Top-5): [['dialect', 'langue', 'français', 'latin', 'accent']]\n",
            "\n",
            "Instance 34:\n",
            "    Tokenized Input: <s> In English, the word is climatic. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: climatique\n",
            "    Predicted MASK words (Top-5): [['climat', 'tropical', 'warm', ':', 'froid']]\n",
            "\n",
            "Instance 35:\n",
            "    Tokenized Input: <s> In English, the word is mountain. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: montagne\n",
            "    Predicted MASK words (Top-5): [['montagne', 'monte', 'mountain', 'terrain', 'château']]\n",
            "\n",
            "Instance 36:\n",
            "    Tokenized Input: <s> In English, the word is central. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: central\n",
            "    Predicted MASK words (Top-5): [['central', 'centrale', 'secondaire', 'latin', 'accent']]\n",
            "\n",
            "Instance 37:\n",
            "    Tokenized Input: <s> In English, the word is alphabet. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: alphabet\n",
            "    Predicted MASK words (Top-5): [['latin', 'lettre', 'alfabet', ':', ':']]\n",
            "\n",
            "Instance 38:\n",
            "    Tokenized Input: <s> In English, the word is vanity. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: vanité\n",
            "    Predicted MASK words (Top-5): [['van', 'constant', ':', 'de', ':']]\n",
            "\n",
            "Instance 39:\n",
            "    Tokenized Input: <s> In English, the word is absurdity. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: absurdité\n",
            "    Predicted MASK words (Top-5): [['absurd', 'excessive', ':', ':', 'bizarre']]\n",
            "\n",
            "Instance 40:\n",
            "    Tokenized Input: <s> In English, the word is giant. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: géant\n",
            "    Predicted MASK words (Top-5): [['grand', 'gros', 'petit', 'king', 'big']]\n",
            "\n",
            "Instance 41:\n",
            "    Tokenized Input: <s> In English, the word is fusion. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: fusion\n",
            "    Predicted MASK words (Top-5): [['fusion', 'Fusion', 'union', 'division', 'dialogue']]\n",
            "\n",
            "Instance 42:\n",
            "    Tokenized Input: <s> In English, the word is heredity. En Français, le mot est<mask> </s>\n",
            "    Expected French word: hérédité\n",
            "    Predicted MASK words (Top-5): [[':', ':', 'the', '.', 'de']]\n",
            "\n",
            "Instance 43:\n",
            "    Tokenized Input: <s> In English, the word is gothic. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: gothique\n",
            "    Predicted MASK words (Top-5): [['latin', 'grec', 'français', 'Greek', 'anglais']]\n",
            "\n",
            "Instance 44:\n",
            "    Tokenized Input: <s> In English, the word is anecdote. En Français, le mot est<mask></s>\n",
            "    Expected French word: anec\n",
            "    Predicted MASK words (Top-5): [['</s>', '.', ':', 'histoire', 'observation']]\n",
            "\n",
            "Instance 45:\n",
            "    Tokenized Input: <s> In English, the word is characteristic. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: caractéris\n",
            "    Predicted MASK words (Top-5): [['character', 'unique', ':', ':', 'distinct']]\n",
            "\n",
            "Instance 46:\n",
            "    Tokenized Input: <s> In English, the word is rigid. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: rigide\n",
            "    Predicted MASK words (Top-5): [['rigid', 'flexible', 'variable', 'stable', 'fixe']]\n",
            "\n",
            "Instance 47:\n",
            "    Tokenized Input: <s> In English, the word is male. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: mâle\n",
            "    Predicted MASK words (Top-5): [['homme', 'femme', 'hommes', 'male', 'femmes']]\n",
            "\n",
            "Instance 48:\n",
            "    Tokenized Input: <s> In English, the word is bureaucracy. En Français, le mot est<mask> </s>\n",
            "    Expected French word: bureaucratie\n",
            "    Predicted MASK words (Top-5): [['bureau', 'administration', ':', ':', 'office']]\n",
            "\n",
            "Instance 49:\n",
            "    Tokenized Input: <s> In English, the word is question. En Français, le mot est<mask> .</s><pad>\n",
            "    Expected French word: question\n",
            "    Predicted MASK words (Top-5): [['question', 'réponse', 'questions', 'problème', 'Question']]\n",
            "\n",
            "Instance 50:\n",
            "    Tokenized Input: <s> In English, the word is detector. En Français, le mot est<mask> .</s>\n",
            "    Expected French word: détecteur\n",
            "    Predicted MASK words (Top-5): [['detect', 'de', ':', ':', 'concentration']]\n",
            "\n",
            "Accuracy of the model: 26.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Model ##"
      ],
      "metadata": {
        "id": "8BGx9YByADkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Load accuracy metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define metric computation function\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy during validation by ignoring padding tokens.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Flatten predictions and labels (remove -100 labels)\n",
        "    flattened_predictions = []\n",
        "    flattened_labels = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        for p, l in zip(pred, label):\n",
        "            if l != -100:       # Ignore padding token labels\n",
        "                flattened_predictions.append(p)\n",
        "                flattened_labels.append(l)\n",
        "\n",
        "    return accuracy.compute(predictions=flattened_predictions, references=flattened_labels)"
      ],
      "metadata": {
        "id": "_m7n4ywIAAr8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define training arguments\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/cognate_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=6,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-6,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        "    fp16=True,\n",
        "    dataloader_drop_last=True,\n",
        "    ignore_data_skip=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=pretrained_model,\n",
        "    args=arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,       # NB: change for test\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bWWSL8L2Arwp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "mppzuBT_BLcA",
        "outputId": "28fe0372-0c51-4dc1-8abc-3fb6eac44e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33/66 02:30 < 02:39, 0.21 it/s, Epoch 2.91/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>17.077400</td>\n",
              "      <td>5.538918</td>\n",
              "      <td>0.371429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>12.079700</td>\n",
              "      <td>4.131742</td>\n",
              "      <td>0.457143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-32-3435b262f1ae>\", line 1, in <cell line: 1>\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2164, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2524, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3654, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3708, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 1211, in forward\n",
            "    outputs = self.roberta(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 977, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 632, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 563, in forward\n",
            "    layer_output = apply_chunking_to_forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 258, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 576, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 487, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 869, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2523\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3653\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3707\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3709\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1212\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/cognate_trainer_best_model\"\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "foKaxwLvBRK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load finetuned model\n",
        "finetuned_model = AutoModelForMaskedLM.from_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "uH0FauszSrrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Finetuned Model ##"
      ],
      "metadata": {
        "id": "b7OyuoZQUpTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of finetuned model\n",
        "accuracy = evaluate_mask_accuracy(finetuned_model, test_dataset, tokenizer, top_k=5)\n",
        "print(f\"Accuracy of the model: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "MkpzXzLxTrbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization ##"
      ],
      "metadata": {
        "id": "j1G1KwYyU02w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "pretrained_model.config.output_hidden_states = True\n",
        "finetuned_model.config.output_hidden_states = True\n",
        "\n",
        "def visualize_embeddings_2D(model, tokenizer, dataset, method=\"pca\", sample_size=50):\n",
        "    \"\"\"\n",
        "    Visualizes word embeddings from the model in 2D space using PCA or t-SNE.\n",
        "\n",
        "    Args:\n",
        "        model: The masked language model (pretrained or fine-tuned).\n",
        "        tokenizer: Tokenizer corresponding to the model.\n",
        "        dataset: Hugging Face Dataset with preprocessed inputs.\n",
        "        method (str): Dimensionality reduction method (\"pca\" or \"tsne\").\n",
        "        sample_size (int): Number of word pairs to sample for visualization.\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    embeddings = []\n",
        "\n",
        "    # Sample a subset for visualization\n",
        "    dataset = dataset.select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    for example in dataset:\n",
        "        input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)  # Add batch dimension\n",
        "        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0)\n",
        "\n",
        "        # Find the <mask> token index\n",
        "        mask_token_index = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        if len(mask_token_index[0]) == 0:\n",
        "            continue  # Skip instances where no <mask> token is found\n",
        "\n",
        "        mask_token_index = mask_token_index[1]  # Extract index positions\n",
        "\n",
        "        # Extract model embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states[-1]  # Get the last hidden layer\n",
        "\n",
        "        # Extract embeddings for <mask> token (mean over all masked positions)\n",
        "        mask_embeddings = hidden_states[0, mask_token_index, :].mean(dim=0).numpy()\n",
        "\n",
        "        embeddings.append(mask_embeddings)\n",
        "\n",
        "        # Store words (English and French)\n",
        "        words.append(f\"{example['word_en']} → {example['word_fr']}\")\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Reduce dimensions\n",
        "    if method == \"pca\":\n",
        "        reducer = PCA(n_components=2)\n",
        "    elif method == \"tsne\":\n",
        "        reducer = TSNE(n_components=2, perplexity=10, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'pca' or 'tsne'.\")\n",
        "\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Plot embeddings\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7, cmap=\"coolwarm\")\n",
        "\n",
        "    # Annotate points with word pairs\n",
        "    for i, word_pair in enumerate(words):\n",
        "        plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], word_pair, fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "    plt.title(f\"2D Projection of Word Embeddings ({method.upper()})\")\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Call function for both models using PCA\n",
        "visualize_embeddings_2D(pretrained_model, tokenizer, test_dataset, method=\"pca\")\n",
        "visualize_embeddings_2D(finetuned_model, tokenizer, test_dataset, method=\"pca\")"
      ],
      "metadata": {
        "id": "VKF4-FGwU0Qz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}